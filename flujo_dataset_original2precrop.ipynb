{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este cuaderno se tiene el flujo completo desde el data set original hasta el data set que se lleva al entrenamiento, esto incluye varios pasos:\n",
    "- Reducir las etiquetas de las imágenes (formato COCO)\n",
    "- Recortar las imágenes según la clase cell phone y mantener el etiquetado original\n",
    "- Aplicar SAHI a las imágenes 128x128\n",
    "- Convertir de Json COCO a YOLO formato\n",
    "- Dividir el conjunto de imágenes entre train, valid y test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Reducir las etiquetas de las imágenes - Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El dataset original viene dividido en varias carpetas con distintos archivos de anotación, el primer paso es fusionar todo, para trabajar con un dataset único."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos combinados y guardados en 'datos_combinados.json'.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Lista de archivos JSON a fusionar\n",
    "archivos_json = ['/home/ana/TFM/datasets/original/dataset_001/annotations/instances_default.json',\n",
    "                   '/home/ana/TFM/datasets/original/dataset_002/annotations/instances_default.json',\n",
    "                     '/home/ana/TFM/datasets/original/dataset_003/annotations/instances_default.json',\n",
    "                       '/home/ana/TFM/datasets/original/dataset_004/annotations/instances_default.json',\n",
    "                         '/home/ana/TFM/datasets/original/dataset_005/annotations/instances_default.json',\n",
    "                           '/home/ana/TFM/datasets/original/dataset_006/annotations/instances_default.json',\n",
    "                             '/home/ana/TFM/datasets/original/dataset_007/annotations/instances_default.json',\n",
    "                               '/home/ana/TFM/datasets/original/dataset_008/annotations/instances_default.json',\n",
    "                                 '/home/ana/TFM/datasets/original/dataset_009/annotations/instances_default.json',\n",
    "                                   '/home/ana/TFM/datasets/original/dataset_010/annotations/instances_default.json' \n",
    "                               ]\n",
    "\n",
    "# Estructuras para almacenar la información combinada\n",
    "imagenes_combinadas = []\n",
    "categorias_combinadas = []\n",
    "anotaciones_combinadas = []\n",
    "\n",
    "# Último id utilizado para asegurar que 'image_id' sea único\n",
    "ultimo_id = 0\n",
    "\n",
    "# Cargar y combinar la información de cada archivo\n",
    "for archivo in archivos_json:\n",
    "    with open(archivo, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        \n",
    "        # Procesar cada imagen para asignar un nuevo 'image_id'\n",
    "        imagenes_temporales = data.get('images', [])\n",
    "        for imagen in imagenes_temporales:\n",
    "            imagen_original_id = imagen['id']\n",
    "            ultimo_id += 1  # Incrementar el último id utilizado\n",
    "            imagen['id'] = ultimo_id  # Asignar el nuevo 'image_id'\n",
    "            \n",
    "            # Actualizar 'image_id' en las anotaciones correspondientes\n",
    "            for anotacion in data.get('annotations', []):\n",
    "                if anotacion['image_id'] == imagen_original_id:\n",
    "                    anotacion['image_id'] = ultimo_id\n",
    "                    anotaciones_combinadas.append(anotacion)  # Agregar la anotación actualizada\n",
    "\n",
    "        # Agregar imágenes actualizadas\n",
    "        imagenes_combinadas.extend(imagenes_temporales)\n",
    "\n",
    "        # Agregar categorías de este archivo\n",
    "        categorias_combinadas.extend(data.get('categories', []))\n",
    "\n",
    "# Eliminar posibles duplicados en categorías si es necesario\n",
    "categorias_unicas = {each['id']: each for each in categorias_combinadas}.values()\n",
    "\n",
    "# Crear un nuevo diccionario con toda la información combinada\n",
    "data_combinada = {\n",
    "    'images': imagenes_combinadas,\n",
    "    'categories': list(categorias_unicas),\n",
    "    'annotations': anotaciones_combinadas\n",
    "}\n",
    "\n",
    "# Guardar la información combinada en un nuevo archivo JSON\n",
    "with open('/home/ana/TFM/datasets/flujo_imagenes _TFM/part1_reduccion_etiquetas/annotations_reduced.json', 'w') as file:\n",
    "    json.dump(data_combinada, file, indent=4)\n",
    "\n",
    "print(\"Datos combinados y guardados en 'datos_combinados.json'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partiendo del dataset unificado tenemos que reducir las clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def eliminar_clases_coco(archivo_json, clases_a_eliminar):\n",
    "    with open(archivo_json, 'r') as f:\n",
    "        datos = json.load(f)\n",
    "\n",
    "    # Crear un diccionario para mapear IDs antiguos a nuevos\n",
    "    id_mapping = {}\n",
    "    nuevo_id = 1\n",
    "    for cat in datos[\"categories\"]:\n",
    "        if cat[\"name\"] not in clases_a_eliminar:\n",
    "            id_mapping[cat[\"id\"]] = nuevo_id\n",
    "            cat[\"id\"] = nuevo_id  # Actualizar el ID en la categoría también\n",
    "            nuevo_id += 1\n",
    "\n",
    "    # Eliminar categorías\n",
    "    datos[\"categories\"] = [cat for cat in datos[\"categories\"] if cat[\"name\"] not in clases_a_eliminar]\n",
    "\n",
    "    # Eliminar anotaciones con categorías eliminadas y actualizar IDs\n",
    "    datos[\"annotations\"] = [\n",
    "        ann for ann in datos[\"annotations\"] if ann[\"category_id\"] in id_mapping\n",
    "    ]\n",
    "    for ann in datos[\"annotations\"]:\n",
    "        ann[\"category_id\"] = id_mapping[ann[\"category_id\"]]\n",
    "\n",
    "    # Guardar el archivo modificado\n",
    "    with open(archivo_json, 'w') as f:\n",
    "        json.dump(datos, f, indent=2)\n",
    "\n",
    "\n",
    "archivo_coco = '/home/ana/TFM/datasets/flujo_imagenes _TFM/part1_reduccion_etiquetas/annotations_reduced.json'  # ruta del archivo con las etiquetas\n",
    "clases_a_eliminar = [\"Camara ara\\u00f1ada\", \n",
    "                     \"Camara leve\", \n",
    "                     \"Camara rota\",\n",
    "                     \"Chasis roto\", \n",
    "                     \"Falta bandeja sim\",\n",
    "                     \"Pantalla profundo\", \n",
    "                     \"Pantalla rota\"]  # Lista de clases a eliminar\n",
    "\n",
    "eliminar_clases_coco(archivo_coco, clases_a_eliminar)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recortar las imágenes según la clase cell phone y mantener el etiquetado original - Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceso completado.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from pathlib import Path\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Configuración inicial\n",
    "# Cargar el modelo YOLO\n",
    "model = YOLO('/home/ana/TFM/modelos/pre-crop-cell-phone/weights/best.pt') #modelo YOLO customizado para detectar el telefono\n",
    "\n",
    "# Rutas de las carpetas y archivos\n",
    "input_folder = '/home/ana/TFM/datasets/flujo_imagenes _TFM/part1_reduccion_etiquetas'\n",
    "output_folder = '/home/ana/TFM/datasets/flujo_imagenes _TFM/part2_crop_cell_phone'\n",
    "coco_file = '/home/ana/TFM/datasets/flujo_imagenes _TFM/part1_reduccion_etiquetas/annotations_reduced.json'\n",
    "\n",
    "output_coco = {'images': [], 'annotations': [], 'categories': []}\n",
    "\n",
    "\n",
    "# Cargar las anotaciones COCO\n",
    "with open(coco_file, 'r') as f:\n",
    "    coco_data = json.load(f)\n",
    "\n",
    "# Preparar mapeo de ID de categoría a nombre de categoría para COCO\n",
    "category_map = {cat['id']: cat['name'] for cat in coco_data['categories']}\n",
    "\n",
    "# Mapear anotaciones COCO por nombre de archivo\n",
    "coco_annotations = {img['file_name']: [] for img in coco_data['images']}\n",
    "\n",
    "# Añadir anotaciones a las imágenes correspondientes\n",
    "for ann in coco_data['annotations']:\n",
    "    image_id = ann['image_id']\n",
    "    image_info = next((img for img in coco_data['images'] if img['id'] == image_id), None)\n",
    "    if image_info:\n",
    "        filename = image_info['file_name']\n",
    "        coco_annotations[filename].append(ann)\n",
    "\n",
    "        \n",
    "# Cargar anotaciones COCO originales\n",
    "with open(coco_file, 'r') as f:\n",
    "    coco_data = json.load(f)\n",
    "output_coco['categories'] = coco_data['categories']\n",
    "\n",
    "# Función para ajustar anotaciones COCO\n",
    "def adjust_annotations(annotations, dx, dy):\n",
    "    adjusted_annotations = []\n",
    "    for ann in annotations:\n",
    "        new_ann = ann.copy()\n",
    "        new_ann['bbox'] = [ann['bbox'][0] - dx, ann['bbox'][1] - dy, ann['bbox'][2], ann['bbox'][3]]\n",
    "        adjusted_annotations.append(new_ann)\n",
    "    return adjusted_annotations\n",
    "\n",
    "# Procesar imágenes\n",
    "image_id = 0\n",
    "annotation_id = 0\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith('.png'):\n",
    "        file_path = os.path.join(input_folder, filename)\n",
    "        image = Image.open(file_path)\n",
    "        results = model(file_path, verbose = False)\n",
    "        image_saved = False\n",
    "        if filename in coco_annotations:\n",
    "            #print('la imagen esta')\n",
    "            for result in results:\n",
    "                boxes = result.boxes.cpu().numpy().xyxy\n",
    "                classes = result.boxes.cpu().numpy().cls\n",
    "                names = result.names\n",
    "                for box, cls in zip(boxes, classes):\n",
    "                    if names[cls] == \"cell phone\":  # Suponiendo que 'cls' contiene el nombre de la clase\n",
    "                        #print('detecto cell phone')\n",
    "                        crop = image.crop((box[0], box[1], box[2], box[3]))\n",
    "                        crop_path = os.path.join(output_folder, f\"{Path(filename).stem}.png\")\n",
    "                        crop.save(crop_path)\n",
    "                        output_coco['images'].append({'id': image_id, 'file_name': os.path.basename(crop_path), 'width': crop.width, 'height': crop.height})\n",
    "                        # Ajustar y añadir anotaciones para el recorte\n",
    "                        adjusted_annotations = adjust_annotations(coco_annotations[filename], box[0], box[1])\n",
    "                        for ann in adjusted_annotations:\n",
    "                            ann['id'] = annotation_id\n",
    "                            ann['image_id'] = image_id\n",
    "                            output_coco['annotations'].append(ann)\n",
    "                            annotation_id += 1\n",
    "                        image_id += 1\n",
    "                        image_saved = True\n",
    "        if not image_saved:\n",
    "\n",
    "            #print('entro a que la imagen no se ha recortado')\n",
    "            original_path = os.path.join(output_folder, filename)\n",
    "            image.save(original_path)\n",
    "            output_coco['images'].append({'id': image_id, 'file_name': os.path.basename(original_path), 'width': image.width, 'height': image.height})\n",
    "            try:\n",
    "                #print(filename)\n",
    "                #print(coco_annotations['IMG_0039_png.rf.7abe8202f5f32488df975de9519b83bc.jpg'])\n",
    "                #print(f\"Desde filename: {coco_annotations[filename]}\")\n",
    "                print(filename)\n",
    "                for ann in coco_annotations[filename]:\n",
    "                    new_ann = ann.copy()  # Trabaja con una copia para evitar modificar el original\n",
    "                    new_ann['id'] = annotation_id\n",
    "                    new_ann['image_id'] = image_id\n",
    "                    output_coco['annotations'].append(new_ann)\n",
    "                    annotation_id += 1\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "            image_id += 1\n",
    "\n",
    "# Guardar el nuevo archivo COCO\n",
    "with open('/home/ana/TFM/datasets/flujo_imagenes _TFM/part2_crop_cell_phone/annotations_cropped.json', 'w') as f:\n",
    "    json.dump(output_coco, f, indent=2)\n",
    "\n",
    "print(\"Proceso completado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicar SAHI a las imágenes - Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indexing coco dataset annotations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading coco annotations: 100%|██████████| 944/944 [00:01<00:00, 623.48it/s]\n",
      "100%|██████████| 944/944 [02:40<00:00,  5.87it/s]\n"
     ]
    }
   ],
   "source": [
    "from sahi.slicing import slice_coco\n",
    "\n",
    "coco_dict, coco_path = slice_coco(\n",
    "    coco_annotation_file_path='/home/ana/TFM/datasets/flujo_imagenes _TFM/part2_crop_cell_phone/annotations_cropped.json',\n",
    "    image_dir='/home/ana/TFM/datasets/flujo_imagenes _TFM/part2_crop_cell_phone/',\n",
    "    output_coco_annotation_file_name='/home/ana/TFM/datasets/flujo_imagenes _TFM/part3_sahi/annotations_sahi.json',\n",
    "    ignore_negative_samples=True,\n",
    "    output_dir='/home/ana/TFM/datasets/flujo_imagenes _TFM/part3_sahi',\n",
    "    slice_height=128,\n",
    "    slice_width=128,\n",
    "    overlap_height_ratio=0.2,\n",
    "    overlap_width_ratio=0.2,\n",
    "    min_area_ratio=0.1,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convertir de JSON COCO a YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El código es externo, de un repositorio de ultralytics: https://github.com/ultralytics/JSON2YOLO, descargar este repositorio y luego hacer uso del archivo general_json2yolo.py, hay que poner como fuente \"COCO\", e introducir la carpeta correcta donde se encuentran las imágenes como en el siguiente ejemplo:\n",
    "\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "    source = \"COCO\"\n",
    "\n",
    "        if source == \"COCO\":\n",
    "            convert_coco_json(\n",
    "                '/home/ana/TFM/datasets/dataset_precrop_sahi_coco',  # directory with *.json\n",
    "                use_segments=False,\n",
    "                cls91to80=True,\n",
    "            )\n",
    "\n",
    "Esto creará en la carpeta donde se encuentre dicho archivo python una carpeta llamada new_dir, que contendrás labels e images, donde se podrá encontrar las etiquetas en formato YOLO (archivos txt con el mismo nombre que sus imágenes asociadas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def mover_contenido(directorio_origen, directorio_destino):\n",
    "    # Lista todos los archivos y directorios en el directorio origen\n",
    "    elementos = os.listdir(directorio_origen)\n",
    "    \n",
    "    for elemento in elementos:\n",
    "        # Construye la ruta completa del elemento en el directorio origen\n",
    "        ruta_completa_origen = os.path.join(directorio_origen, elemento)\n",
    "        \n",
    "        # Mueve cada elemento al directorio destino\n",
    "        shutil.move(ruta_completa_origen, directorio_destino)\n",
    "\n",
    "# Rutas de los directorios\n",
    "directorio_origen = '/home/ana/JSON2YOLO/new_dir/labels'\n",
    "directorio_destino = '/home/ana/TFM/datasets/flujo_imagenes _TFM/part4_yolo_format'\n",
    "\n",
    "# Llama a la función para mover el contenido\n",
    "mover_contenido(directorio_origen, directorio_destino)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividir el conjunto de imágenes (train/valid/test) - Part 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los archivos han sido distribuidos correctamente en las carpetas de train, valid y test.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from random import shuffle\n",
    "import yaml\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "# Rutas a las carpetas donde se encuentran todas las imágenes y etiquetas\n",
    "carpeta_imagenes = '/home/ana/TFM/datasets/flujo_imagenes _TFM/part3_sahi'\n",
    "carpeta_labels = '/home/ana/TFM/datasets/flujo_imagenes _TFM/part4_yolo_format/annotations_sahi.json_coco'\n",
    "\n",
    "# Crea las carpetas para los conjuntos de datos\n",
    "sets = ['train', 'valid', 'test']\n",
    "for set_name in sets:\n",
    "    os.makedirs(f'/home/ana/TFM/datasets/flujo_imagenes _TFM/part5_split/{set_name}/images', exist_ok=True)\n",
    "    os.makedirs(f'/home/ana/TFM/datasets/flujo_imagenes _TFM/part5_split/{set_name}/labels', exist_ok=True)\n",
    "\n",
    "# Lista todas las imágenes y etiquetas\n",
    "all_images = [f for f in os.listdir(carpeta_imagenes) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "all_labels = [f for f in os.listdir(carpeta_labels) if f.endswith('.txt')]\n",
    "\n",
    "# Asegúrate de que cada imagen tenga su correspondiente archivo de etiqueta\n",
    "all_files = [(img, img.replace('.png', '.txt')) for img in all_images if img.replace('.png', '.txt') in all_labels]\n",
    "\n",
    "# Baraja aleatoriamente los archivos\n",
    "shuffle(all_files)\n",
    "\n",
    "# Asigna archivos a cada conjunto\n",
    "train_files = all_files[:10000]\n",
    "valid_files = all_files[10000:12000]\n",
    "test_files = all_files[12000:14000]\n",
    "\n",
    "# Función para mover los archivos\n",
    "def move_files(files, set_name):\n",
    "    for img, label in files:\n",
    "        # Mover imágenes\n",
    "        shutil.move(os.path.join(carpeta_imagenes, img), f'/home/ana/TFM/datasets/flujo_imagenes _TFM/part5_split/{set_name}/images/{img}')\n",
    "        # Mover etiquetas\n",
    "        shutil.move(os.path.join(carpeta_labels, label), f'/home/ana/TFM/datasets/flujo_imagenes _TFM/part5_split/{set_name}/labels/{label}')\n",
    "\n",
    "# Mueve los archivos a las carpetas correspondientes\n",
    "move_files(train_files, 'train')\n",
    "move_files(valid_files, 'valid')\n",
    "move_files(test_files, 'test')\n",
    "\n",
    "print(\"Los archivos han sido distribuidos correctamente en las carpetas de train, valid y test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
